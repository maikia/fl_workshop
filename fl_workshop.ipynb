{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "873374cb-1816-409a-9e68-12cb869e7fde",
   "metadata": {},
   "source": [
    "logos: owkin, Pyladies Paris, Substra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c298237-56d0-4711-be8f-cffec99a69e0",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Helper\n",
    "(in separate notebook)\n",
    "-> what you need to have installed (conda? libraries, jupyter lab)\n",
    "-> few helpful funcions about jupyter lab\n",
    "-> knowledge on deep learning algorithms is not required but will be useful. This said, this workshop is very new so any comments are apprepriated. Please stop me at any time if something is not clear.\n",
    "-> If you wish to learn more about ML refer to: (TODO: references)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb9cd51-1c09-4fe4-bcf1-c5cdabb5d79d",
   "metadata": {},
   "source": [
    "# Introduction to Federated Learning\n",
    "\n",
    "**author**: Maria Telenczuk\n",
    "\n",
    "TODO: few words about federated learning (FL)\n",
    "\n",
    "### Resources\n",
    "\n",
    "1. 1st article on FL:\n",
    "    [H. Brendan McMahan and Eider Moore and Daniel Ramage and Seth Hampson and Blaise Ag√ºera y Arcas (2017) Communication-Efficient Learning of Deep Networks from Decentralized Data, arXiv1602.05629](https://arxiv.org/abs/1602.05629)\n",
    "1. [Google blogpost on FL](https://ai.googleblog.com/2017/04/federated-learning-collaborative.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a216d0e-4061-4246-9166-a5cff51feb88",
   "metadata": {},
   "source": [
    "## Quick intro to Machine Learning\n",
    "\n",
    "**Machine learning** TODO: general idea\n",
    "\n",
    "**ML algorithms** TODO: genaral idea\n",
    "\n",
    "**Deep learning algorithms** TODO: general idea with some image, explain what are weights and gradients and hyperparameters.\n",
    "\n",
    "**Data** TODO: importance of the data in ML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86fd952-ed15-4159-982f-c20ba249bcc0",
   "metadata": {},
   "source": [
    "## Import all necessary libraries\n",
    "\n",
    "We are using Pytorch (libary which simplifies creating deep learning algorithms)\n",
    "and matplotlib (which we will use for plotting the data).\n",
    "%matplotlib inline is \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c7f78-cf91-4b7c-a1c3-795a8c6ba1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mnist dataset + understand it\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93646d15-1fe5-49be-a2a9-985173d2c201",
   "metadata": {},
   "source": [
    "## prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c718c69e-4f4a-481c-a262-113b42aa0058",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "# n_train_data = len(training_data)\n",
    "# n_val_data = int(n_train_data * 0.3)\n",
    "\n",
    "# creat train, validate and test dataloaders\n",
    "# train_alone, validate_dataset = torch.utils.data.random_split(training_data, [n_train_data - n_val_data, n_val_data])\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "# validate_dataloader = DataLoader(validate_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "\n",
    "#for X, y in test_dataloader:\n",
    "#    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "#    print(\"Shape of y: \", y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a2a4c1-8731-4dc4-9e09-e9eea8092dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(8, 8))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    image, label = training_data[i]\n",
    "    ax.imshow(image.squeeze(), cmap=\"gray\", interpolation='nearest')\n",
    "    ax.text(0.05, 0.05, str(label),\n",
    "            transform=ax.transAxes, color='white')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d6933-91eb-4818-99e7-813e7558bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    0: 't-shirt',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle boot'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6768463e-caaa-4da6-99f7-dbd2eca72501",
   "metadata": {},
   "source": [
    "## Prepare algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f266757-71aa-4d84-8070-6a6572368d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, lambda_dropout=0.0):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        n_hidden = 32\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.Dropout(lambda_dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, 10),\n",
    "            # nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de9ce2c-c4bf-4c92-916e-118205ec676b",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ffcf5-bdd9-4435-8be5-ca109a485dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        image, label = data\n",
    "        if image.shape[0] < 64:\n",
    "            continue\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(image)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % batch_size == batch_size-1:    # print every 50 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 50))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09ce017-a180-40c7-8941-048bc6e9b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    prediction = []\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_dataloader:\n",
    "            # image, label = X.to(self._device), y.to(self._device)\n",
    "            pred = model(image)\n",
    "\n",
    "            # to view predicted labels uncomment following lines:\n",
    "            # print('predicted:', pred.argmax(1))\n",
    "            # print('true:',label)\n",
    "            prediction.append(pred)\n",
    "    return prediction\n",
    "\n",
    "prediction = predict_model(model, test_dataloader)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed761d-a884-4789-a9e9-132ac475c6d2",
   "metadata": {},
   "source": [
    "## calculate the score given the data and the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966a037c-a9d5-45f5-9ad9-f33961ce0b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(dataloader, next_pred):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    for [X, y], y_pred in zip(dataloader, next_pred):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        test_loss += loss_fn(y_pred, y).item()\n",
    "        correct += (y_pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    accuracy = (100*correct)\n",
    "    avg_loss = test_loss\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "accuracy, loss = get_score(test_dataloader, prediction)\n",
    "\n",
    "print('accuracy is:', accuracy)\n",
    "print('loss is:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8aafc1-0f9f-4bd4-87c6-7111ff800bde",
   "metadata": {},
   "source": [
    "You can predict the output based on this data\n",
    "\n",
    "Now, we will define a simple pytorch model to train this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d871ac7-cc9b-4743-b9b3-fcb844fa8603",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "print(model)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc61bdc-50d2-4d78-bf8a-344f2688d475",
   "metadata": {},
   "source": [
    "# Introduction to ML\n",
    "- data, mnistfashion or other\n",
    "- simple model\n",
    "- privacy. give examples: (personal data, phone/e-mail data, healthcare data etc)\n",
    "# show an image on classical ML\n",
    "# privacy issues\n",
    "# explain the concept of FL\n",
    "# FL challenges (performance and privacy, amount of data, trust of the algorithm if not seen, trust of the data of others etc\n",
    "This very much differs on the use case. The first article (give reference, google) describing the concept of FL was provided by Google for their usecase. A lot of phone users write their messages and it would be great to be able to suggest to them the rest of the message. It can be very personal what you write so the suggestion should be personalized, but you most likely won't provide enough data to train the deep learnig algorithm. On the other side it would be a security breach if all the messages, from all the phones were send to the single server for training. Google proposed that the model will be trained on each phone for one or more epochs and then the model, not the data will be send to the centralized location. This could be for example weights. In the centralized location the server then combines those weights, for example averages them and sends back to each of the phones. Those updated weights they then use for further training.\n",
    "\n",
    "In Owkin the use case is very different. The clients or sites are only very few but they have much larger data. The data is usually very sensitive and no information can leave the site. Nobody should even know how much of the data is on which site. You won't be able to check if it is well preprocessed or for the missing data so as you imagine it comes with additional challenges and I suppose, in all cases, some trust.\n",
    "\n",
    "Another challenge is a performance. The bottleneck of FL is communication: sending the model back and fourth and waiting until the processing is finished. Especially if the model is very big, and even more so if there are a lot of sites.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c837ff-9fe4-4ce2-86be-2025ac78b57e",
   "metadata": {},
   "source": [
    "# so let's now imagine..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59712e73-4a73-4b33-9087-886dd41bdaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We divide the data to 3 sites\n",
    "n_train_data = len(training_data)\n",
    "n_site1, n_site2 = int(n_train_data * 0.5), int(n_train_data * 0.2)\n",
    "n_site3 = n_train_data - n_site1 - n_site2\n",
    "site1, site2, site3 = torch.utils.data.random_split(training_data, [n_site1, n_site2, n_site3])\n",
    "site1_dataloader = DataLoader(site1, batch_size=batch_size, shuffle=True)\n",
    "site2_dataloader = DataLoader(site2, batch_size=batch_size, shuffle=True)\n",
    "site3_dataloader = DataLoader(site3, batch_size=batch_size, shuffle=True)\n",
    "print(len(site1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b049b71-af9c-4860-8559-a552e9a4ff15",
   "metadata": {},
   "source": [
    "## update our model to fl settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e3481b-899d-44af-a18a-a2006e0f3007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "model = NeuralNetwork()\n",
    "learning_rate = 0.1\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, dataloader):\n",
    "    # note: this is very simplified and won't work with every optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        num_batches = len(dataloader)\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        image, label = data\n",
    "        if image.shape[0] < 64:\n",
    "            continue\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(image)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % batch_size == batch_size-1:\n",
    "            #print(loss.item())\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / batch_size))\n",
    "            running_loss = 0.0\n",
    "    return model\n",
    "\n",
    "\n",
    "def avg_weights(weight1, weight2, weight3):\n",
    "    \n",
    "    mean_weights = []\n",
    "    for w1, w2, w3 in zip(weight1, weight2, weight3):\n",
    "        next_weight = (w1 + w2 + w3) / 3\n",
    "        #next_weight = torch.mean(w1, w2, w3)\n",
    "        # loop through all the weights\n",
    "        #for i in range(1, len(weights)):\n",
    "        #    next_weight += weight\n",
    "        #next_weigth /= idx\n",
    "        mean_weights.append(next_weight)\n",
    "    return mean_weights\n",
    "    \n",
    "\n",
    "def get_weights(model):\n",
    "    return model.parameters()\n",
    "\n",
    "def set_weights(model, new_weights):\n",
    "    with torch.no_grad():\n",
    "        for param, new_param in zip(model.parameters(), new_weights):\n",
    "            param.data[:] = new_param.data\n",
    "    return model\n",
    "    \n",
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "    \n",
    "    # train on site1\n",
    "    # beta = 0.5 #The interpolation parameter\n",
    "    '''\n",
    "    params1 = model1.named_parameters()\n",
    "    params2 = model2.named_parameters()\n",
    "\n",
    "    dict_params2 = dict(params2)\n",
    "\n",
    "    for name1, param1 in params1:\n",
    "        if name1 in dict_params2:\n",
    "            dict_params2[name1].data.copy_(beta*param1.data + (1-beta)*dict_params2[name1].data)\n",
    "\n",
    "     model.load_state_dict(dict_params2)\n",
    "    ''' \n",
    "    \n",
    "    model1 = train(copy.deepcopy(model), site1_dataloader)\n",
    "    model2 = train(copy.deepcopy(model), site2_dataloader)\n",
    "    model3 = train(copy.deepcopy(model), site3_dataloader)\n",
    "\n",
    "    params_m1 = get_weights(model1)\n",
    "    params_m2 = get_weights(model2)\n",
    "    params_m3 = get_weights(model3)\n",
    "    \n",
    "    mean_weights = avg_weights(params_m1, params_m2, params_m3)\n",
    "    \n",
    "    model = set_weights(model, mean_weights)\n",
    "    print('continue to epoch, ', epoch)\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b4028-05db-4a35-893b-150115519d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "prediction = predict_model(model, test_dataloader)\n",
    "accuracy, loss = get_score(test_dataloader, prediction)\n",
    "\n",
    "print('accuracy is:', accuracy)\n",
    "print('loss is:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271b6ee5-7947-4bb1-bbf3-80b8aa0d67ee",
   "metadata": {},
   "source": [
    "## Security"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dc2195-f92a-4cd2-9ed5-ad54fc2756db",
   "metadata": {},
   "source": [
    "## hacking federated learning\n",
    "If we just left the model as it is it would have been easy to hack on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b7376d-3755-4006-858b-b4f404dd3b80",
   "metadata": {},
   "source": [
    "Now we will use slightly different data for site1 and site2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9541d8-eccd-4a87-a407-e7fdd4428063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision as torchvision\n",
    "import torch.utils.data.sampler as sampler\n",
    "def get_indices(dataset, class_name):\n",
    "    indices =  []\n",
    "    for i in range(len(dataset.dataset.targets)):\n",
    "        if dataset.dataset.targets[i] in class_name:\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "\n",
    "\n",
    "#dataset = torchvision.datasets.CIFAR10(root='./data',\n",
    "#                           transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "idx = get_indices(site1, [0, 1, 2])\n",
    "site1_newloader = DataLoader(training_data, batch_size=64, sampler=sampler.SubsetRandomSampler(idx))\n",
    "\n",
    "# idx = get_indices(site2, [5, 6])\n",
    "# site2_newloader = DataLoader(site2, batch_size=64, sampler=sampler.SubsetRandomSampler(idx))\n",
    "\n",
    "for idx, (data, target) in enumerate(site1_newloader):\n",
    "    pass\n",
    "print(idx, target)\n",
    "# for idx, (data, target) in enumerate(site2_newloader):\n",
    "#    print(idx, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e94d4ba-5c7a-4876-a52d-b7f2d4641220",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = train(copy.deepcopy(model), site1_newloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64c3442-7ccc-4997-bc40-75610c20e239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b3e66e-8ace-4992-8179-e1bf1af0e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image, label = len(training_data)\n",
    "idx = 0\n",
    "\n",
    "*_, last_m1 = model1.parameters() # for a better understanding check PEP 448\n",
    "*_, last_m = model.parameters() \n",
    "# print(last)\n",
    "\n",
    "\n",
    "for idx, weight in enumerate(last_m1 - last_m):\n",
    "    # m = m1.data - m2.data\n",
    "    #if idx == 5:\n",
    "    #    print(m1, m2)\n",
    "    #    print(m)\n",
    "    if weight > 0:\n",
    "        print(idx, ':', classes[idx])\n",
    "    #if i > 3:\n",
    "    #    break\n",
    "    # print(v.detach().numpy().shape)\n",
    "    #plt.imshow(v.detach().numpy())\n",
    "    # to view predicted labels uncomment following lines:\n",
    "    # print('predicted:', pred.argmax(1))\n",
    "    # print('true:',label)\n",
    "    #if(v != 0):\n",
    "    #     print(vocab[i])\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01f2ecd-85b1-4c93-a191-029c24513911",
   "metadata": {},
   "source": [
    "So in fact we don't want the \n",
    "\n",
    "Explain secure aggregation\n",
    "\n",
    "- adding noise (more secure but could harm performance)\n",
    "- so aggregate in a way that nobody can view the weights of others\n",
    "- so how could we do that? use homomorphic encryption\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab12ca8-8d8f-4bab-ae56-1b64618e765b",
   "metadata": {},
   "source": [
    "## What is homomorphic encryption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad06c62-e157-48ca-bcf8-60dca6de4140",
   "metadata": {},
   "source": [
    "homomorphic encryption: gives possibility to perform some computations on encrypted values without decrypting them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082ea63e-3a1b-4c4f-b8bd-c42344769822",
   "metadata": {},
   "source": [
    "First, a public key lets you encrypt numbers. A private key lets you decrypt\n",
    "encrypted numbers. An encrypted value is called a ciphertext, and an unencrypted value\n",
    "is called a plaintext. \n",
    "\n",
    "phe library? or something used in Substra?\n",
    "give some resources\n",
    "\n",
    "Now, let‚Äôs return to the problem of secure aggregation. Given your new knowledge that\n",
    "you can add together numbers you can‚Äôt see, the answer becomes plain. The person who\n",
    "initializes the model sends a public_key to Bob, Alice, and Sue so they can each encrypt\n",
    "their weight updates\n",
    "\n",
    "Then, Bob, Alice, and Sue (who don‚Äôt have the private key) talk directly\n",
    "to each other and accumulate all their gradients into a single, final update that‚Äôs sent back to\n",
    "the model owner, who decrypts it with the private_key.\n",
    "\n",
    "In substra: Encryption function using python's pre-installed packages HMAC and hashlib.\n",
    "    We are using SHA256 (it is equal in security to SHA512)\n",
    "    scaffold (??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21217272-1e5f-46df-88dd-9f8a4ac2c9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model = Embedding(vocab_size=len(vocab), dim=1)\n",
    "model.weight.data *= 0\n",
    "\n",
    "# note that in production the n_length should be at least 1024\n",
    "public_key, private_key = phe.generate_paillier_keypair(n_length=128)\n",
    "\n",
    "def train_and_encrypt(model, input, target, pubkey):\n",
    "    new_model = train(copy.deepcopy(model), input, target, iterations=1)\n",
    "\n",
    "    encrypted_weights = list()\n",
    "    for val in new_model.weight.data[:,0]:\n",
    "        encrypted_weights.append(public_key.encrypt(val))\n",
    "    ew = np.array(encrypted_weights).reshape(new_model.weight.data.shape)\n",
    "    \n",
    "    return ew\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103862c7-ea95-4857-a984-faf8ad7e59ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# enable encrypting\n",
    "\n",
    "import syft.frameworks.tenseal as ts\n",
    "\n",
    "# hook PyTorch to add extra functionalities like the ability to encrypt torch tensors\n",
    "hook = sy.TorchHook(th)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b0eba-08d5-44db-9af8-7456b5299376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import phe\n",
    "\n",
    "import syft as sy\n",
    "\n",
    "# hook PyTorch to add extra functionalities like the ability to encrypt torch tensors\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "model = NeuralNetwork()\n",
    "learning_rate = 0.1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# this should be more secure for production\n",
    "public_key, private_key = phe.generate_paillier_keypair(n_length=128)\n",
    "\n",
    "def train_and_encrypt(model, dataloader, public_key):\n",
    "    # note: this is very simplified and won't work with every optimizer\n",
    "    \n",
    "    new_model = train(copy.deepcopy(model), dataloader)\n",
    "    \n",
    "    encrypted_layers = list()\n",
    "    for val in new_model.parameters():\n",
    "        encrypted_weights = list()\n",
    "        # matrix_encrypted = val.data.encrypt(\"ckks\", public_key=public_keys)\n",
    "        # data = val.data[:,0]\n",
    "        # print(val.data[:,0])\n",
    "        # print('vs', val.data)\n",
    "        for num in val:\n",
    "            print(num)\n",
    "            encrypted_weights.append(public_key.encrypt(num))\n",
    "        enc_weights = np.array(encrypted_layers).reshape(new_model.weight.data.shape)\n",
    "        encrypted_layers.append(encrypted_weights)\n",
    "    \n",
    "    return enc_weights\n",
    "\n",
    "\n",
    "def avg_weights(weight1, weight2, weight3):\n",
    "    \n",
    "    mean_weights = []\n",
    "    weight1 + weight2 + weight3\n",
    "    for w1, w2, w3 in zip(weight1, weight2, weight3):\n",
    "        next_weight = (w1 + w2 + w3) / 3\n",
    "        mean_weights.append(next_weight)\n",
    "    return mean_weights\n",
    "    \n",
    "\n",
    "def get_weights(model):\n",
    "    return model.parameters()\n",
    "\n",
    "def set_weights(model, new_weights):\n",
    "    with torch.no_grad():\n",
    "        for param, new_param in zip(model.parameters(), new_weights):\n",
    "            param.data[:] = new_param.data\n",
    "    return model\n",
    "    \n",
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "    \n",
    "    w1 = train_and_encrypt(copy.deepcopy(model), site1_dataloader, public_key)\n",
    "    w2 = train_and_encrypt(copy.deepcopy(model), site2_dataloader, public_key)\n",
    "    w3 = train_and_encrypt(copy.deepcopy(model), site3_dataloader, public_key)\n",
    "\n",
    "    \n",
    "    mean_weights = avg_weights(w1, w2, w3)\n",
    "    \n",
    "    model = set_weights(model, mean_weights)\n",
    "    print('continue to epoch, ', epoch)\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe44bd0-8d33-4d76-9114-4f50172849d3",
   "metadata": {},
   "source": [
    "other concepts:  federated learning,\n",
    "homomorphic encryption, differential privacy, and secure multi-party computation are all\n",
    "built in "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38de22bd-86e5-40b9-9a92-581e054a05ff",
   "metadata": {},
   "source": [
    "Two words about Substra and Connect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6895a703-6b5e-4e9c-b3a5-ee204aa088c9",
   "metadata": {},
   "source": [
    "different strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b7c0c3-7296-40f0-b813-162a82ea576a",
   "metadata": {},
   "source": [
    "two words about Connectlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041035f0-14c2-4cb2-964a-838fdc2fe594",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
